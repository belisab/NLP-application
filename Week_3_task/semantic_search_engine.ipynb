{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3c9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f220ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "ft = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b38a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in the list is: 1225 Type of documents: <class 'str'>\n",
      "First 500 characters of the first document:\n",
      " Hello fellow Wikipedians,\n",
      "\n",
      "I have just modified one external link on 10th edition of Systema Naturae . Please take a moment to review my edit . If you have any questions, or need the bot to ignore the links, or the page altogether, please visit this simple FaQ for additional information. I made the following changes:\n",
      "\n",
      "When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.\n",
      "\n",
      "This message was posted before February 2018. A\n"
     ]
    }
   ],
   "source": [
    "# Processing multiple HTML files in a folder\n",
    "folder_path = r\"wikipedia_talk_pages\"\n",
    "documents = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".html\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "            result = []\n",
    "\n",
    "            # page title \n",
    "            title = soup.find(\"span\", class_=\"mw-page-title-main\")\n",
    "            if title:\n",
    "                result.append(title.get_text(strip=True)) # strip removes leading/trailing whitespace\n",
    "\n",
    "            # main content\n",
    "            root = soup.find(\"div\", id=\"mw-content-text\") # we create root to be sure we are in the content section\n",
    "            content = root.find(\"div\", class_=\"mw-parser-output\") if root else None # it verifies root is not None\n",
    "            # in content, we look for h2, h3, and p tags that are direct children (not nested deeper)\n",
    "\n",
    "            if content:\n",
    "                for el in content.find_all([\"h2\", \"h3\", \"p\"], recursive=False): # recursive=False ensures we only get direct children\n",
    "                    text = el.get_text(\" \", strip=True)\n",
    "                    if text:\n",
    "                        result.append(text)\n",
    "\n",
    "            full_text = \"\\n\\n\".join(result) # every wiki page is stored as a single string with double newlines between sections\n",
    "            if full_text:   # only add non-empty documents    \n",
    "                documents.append(full_text)\n",
    "\n",
    "print(f'The number of documents in the list is: {len(documents)}', \"Type of documents:\", type(documents[0]))\n",
    "print(\"First 500 characters of the first document:\\n\", documents[0][:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c644a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = gensim.models.keyedvectors.KeyedVectors(ft.vector_size, count=len(documents))\n",
    "for i, line in enumerate(documents):\n",
    "    # gensim provides procedures for preprocessing and stopword removal\n",
    "    text_without_stopwords = gensim.parsing.preprocessing.remove_stopwords(line)\n",
    "    tokens = gensim.utils.simple_preprocess(text_without_stopwords)\n",
    "    # the function get_mean_vector computes the average vector for all tokens\n",
    "    dv = ft.get_mean_vector(tokens)\n",
    "    doc_vectors.add_vector(i, dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc60d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
